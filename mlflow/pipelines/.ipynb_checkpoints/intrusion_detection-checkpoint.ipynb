{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b508aeb4-2ef0-4bc3-9a24-3eb558d199e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    InputTextFile,\n",
    "    OutputPath,\n",
    "    OutputTextFile,\n",
    "    func_to_container_op,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from typing import NamedTuple\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from constants import NAMESPACE, HOST, NAMESPACE\n",
    "from utils import get_session_cookie, get_or_create_experiment, get_or_create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157f76b4-8d6c-44db-b8cc-355ff91ed509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where all the runs belong to the pipeline reside in\n",
    "EXPERIMENT_NAME = \"mle-3-intrusion-detection-training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad1788-5a9c-4477-a5e8-f016a2eea8ea",
   "metadata": {},
   "source": [
    "## Define pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffe13f3-8d6b-4c94-bcbb-aa3238942e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "\n",
    "# The first component to download data, train-test split\n",
    "# and then dump all the data for downstream components to use\n",
    "def prepare_data(\n",
    "    X_train_path: OutputPath(\"PKL\"),\n",
    "    y_train_path: OutputPath(\"PKL\"),\n",
    "    mean_path:  OutputPath(\"MEAN\"),\n",
    "    stdev_path: OutputPath(\"STDEV\"),\n",
    ") -> str:\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import random\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(constants.TRACKING_URI) # Define which tracking server to use\n",
    "\n",
    "    from alibi_detect import datasets\n",
    "    from alibi_detect.utils.data import create_outlier_batch\n",
    "\n",
    "    # Load the dataset from alibi_detect\n",
    "    intrusions = datasets.fetch_kdd()\n",
    "\n",
    "    # Set seed to ensure reproducibility\n",
    "    def seed_everything(seed: int):\n",
    "        random.seed(seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    seed_everything(42)\n",
    "\n",
    "    # Create X_train and y_train\n",
    "    logging.info(\"Creating data...\")\n",
    "    n_samples = 4000\n",
    "    normal_batch = create_outlier_batch(\n",
    "        intrusions.data, intrusions.target, n_samples=n_samples, perc_outlier=0\n",
    "    )\n",
    "    X_train, y_train = normal_batch.data.astype(float), normal_batch.target\n",
    "\n",
    "    # Preprocess X_train and y_train\n",
    "    mean, stdev = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "    X_train = (X_train - mean) / stdev\n",
    "\n",
    "    # Dump data to pkl for downstream components to use\n",
    "    logging.info(\"Dumping data...\")\n",
    "    joblib.dump(X_train, X_train_path)\n",
    "    joblib.dump(y_train, y_train_path)\n",
    "    joblib.dump(mean, mean_path)\n",
    "    joblib.dump(stdev, stdev_path)\n",
    "\n",
    "    # Log all artifacts to MLFlow\n",
    "    # There are many ways to log. from log_param, log_metric, \n",
    "    # to log_artifact and log_artifacts, please refer to https://mlflow.org/docs/latest/tracking/tracking-api.html#logging-functions\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_artifact(X_train_path)\n",
    "        mlflow.log_artifact(y_train_path)\n",
    "        mlflow.log_artifact(mean_path)\n",
    "        mlflow.log_artifact(stdev_path)\n",
    "\n",
    "    # Get the run_id to pass to the following step\n",
    "    # so that 2 steps can log into the same run\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    return run_id\n",
    "\n",
    "# Instead of using create_component_from_func,\n",
    "# you can use this instead\n",
    "prepare_data_op = func_to_container_op(\n",
    "    func=prepare_data,\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"alibi-detect[tensorflow]==0.11.1\",\n",
    "        \"cloudpickle==2.1.0\",\n",
    "        \"kfp==1.8.22\",\n",
    "        \"urllib3==1.26.15\",\n",
    "        \"requests-toolbelt==0.10.1\",  # To fix ImportError: cannot import name 'appengine' from 'urllib3.contrib'\n",
    "        \"mlflow==2.9.2\"\n",
    "    ],\n",
    "    modules_to_capture=[\n",
    "        \"constants\",\n",
    "    ],\n",
    "    use_code_pickling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6339e7c-b657-4049-a57c-8369b70a6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 12:29:35.890597: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 12:29:36.009766: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 12:29:36.517598: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2023-12-27 12:29:36.517666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2023-12-27 12:29:36.517672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/quandv/anaconda3/envs/km/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import intrusion_detection_model\n",
    "\n",
    "# The 2nd component receives outputs from the 1st component\n",
    "# and train\n",
    "def train(\n",
    "    run_id: str,\n",
    "    X_train_path: InputPath(\"PKL\"),\n",
    "    y_train_path: InputPath(\"PKL\"),\n",
    "    vae_output_path: OutputPath(\"VAE\"),\n",
    ") -> NamedTuple('DummyOutputs', [('run_id', str), ('n_features', int)]):\n",
    "    import joblib\n",
    "    \n",
    "    from alibi_detect.models.tensorflow import elbo\n",
    "    \n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(constants.TRACKING_URI) # Define which tracking server to use\n",
    "    \n",
    "    # Load data from the previous step\n",
    "    X_train = joblib.load(X_train_path)\n",
    "    y_train = joblib.load(y_train_path)\n",
    "\n",
    "    # Train a VAE model\n",
    "    n_features = X_train.shape[1]\n",
    "    latent_dim = 2\n",
    "    vae_trainer = intrusion_detection_model.Trainer(\"vae\", n_features, latent_dim)\n",
    "\n",
    "    # Pay attention here, we use the output run_id \n",
    "    # of the last step before logging\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        # Define some params for training\n",
    "        perc_outlier = 5\n",
    "        \n",
    "        vae_trainer.train(\n",
    "            X_train,\n",
    "            perc_outlier=perc_outlier,\n",
    "            loss_fn=elbo,\n",
    "            cov_elbo=dict(sim=0.01),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "        )\n",
    "        # We want to log some params to MLFlow\n",
    "        mlflow.log_param(\"perc_outlier\", 5)\n",
    "    \n",
    "        # Save the model for prediction\n",
    "        vae_trainer.save_model(vae_output_path)\n",
    "\n",
    "        # We also want to save the model to MLFlow,\n",
    "        # unluckily, MLFLow does not support saving a VAE model directly in https://mlflow.org/docs/latest/models.html#built-in-model-flavors.\n",
    "        # Therefore, you can use mlflow.pyfunc.log_model for your custom model, or simple mlflow.log_artifacts as follows\n",
    "        mlflow.log_artifacts(vae_output_path)\n",
    "\n",
    "        # and create a fake pyfunc model, so that you can register the model\n",
    "        # , if you don't do it, you can not use the `register model` function of mlflow\n",
    "        class MyModel(mlflow.pyfunc.PythonModel):\n",
    "            def predict(self, context, model_input, params=None):\n",
    "                return None\n",
    "                \n",
    "        mlflow.pyfunc.log_model(artifact_path=\"model\", python_model=MyModel())\n",
    "\n",
    "        # Continue to pass run_id to the following step\n",
    "        return run_id, X_train.shape[1]\n",
    "\n",
    "train_op = func_to_container_op(\n",
    "    func=train,\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"alibi-detect[tensorflow]==0.11.1\",\n",
    "        \"alibi==0.9.1\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "        \"joblib==1.3.2\",\n",
    "        \"cloudpickle==2.1.0\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"kfp==1.8.22\",\n",
    "        \"urllib3==1.26.15\",\n",
    "        \"requests-toolbelt==0.10.1\",  # To fix ImportError: cannot import name 'appengine' from 'urllib3.contrib'\n",
    "        \"mlflow==2.9.2\"\n",
    "    ],\n",
    "    modules_to_capture=[\n",
    "        \"intrusion_detection_model\",\n",
    "        \"constants\"\n",
    "    ],  # https://kubeflow-pipelines.readthedocs.io/en/1.8.13/source/kfp.components.html#kfp.components.func_to_container_op\n",
    "    use_code_pickling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20498b63-06e1-40f5-adcf-5cddfd8f9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3rd component receives outputs from the 2nd component\n",
    "# in combination with prediction data from the 1st component\n",
    "# to evaluate the model\n",
    "def evaluate(\n",
    "    run_id: str,\n",
    "    n_features: int,\n",
    "    mean_path: InputPath(\"MEAN\"),\n",
    "    stdev_path: InputPath(\"STDEV\"),\n",
    "    vae_output_path: InputPath(\"VAE\"),\n",
    ") -> NamedTuple(\"Outputs\", [(\"mlpipeline_metrics\", \"Metrics\"),]):\n",
    "    from alibi_detect import datasets\n",
    "    from alibi_detect.utils.data import create_outlier_batch\n",
    "\n",
    "    import joblib\n",
    "    import json\n",
    "\n",
    "    import mlflow\n",
    "    mlflow.set_tracking_uri(constants.TRACKING_URI) # Define which tracking server to use\n",
    "\n",
    "    # Load the dataset from alibi_detect\n",
    "    intrusions = datasets.fetch_kdd()\n",
    "    \n",
    "    # Generate an evaluation batch, and create evaluation data\n",
    "    perc_outlier = 5\n",
    "    n_samples = 10000\n",
    "\n",
    "    # Create a batch of outlier for testing purposes\n",
    "    outlier_batch = create_outlier_batch(\n",
    "        intrusions.data,\n",
    "        intrusions.target,\n",
    "        n_samples=n_samples,\n",
    "        perc_outlier=perc_outlier,\n",
    "    )\n",
    "\n",
    "    # Get features and target\n",
    "    X_test, y_test = outlier_batch.data.astype(float), outlier_batch.target\n",
    "\n",
    "    # Normalize based on mean and stdev from the training set\n",
    "    mean = joblib.load(mean_path)\n",
    "    stdev = joblib.load(stdev_path)\n",
    "    X_test = (X_test - mean) / stdev\n",
    "\n",
    "    # Again, specify hyper-params to construct our network\n",
    "    latent_dim = 2\n",
    "\n",
    "    vae_trainer = intrusion_detection_model.Trainer(\"vae\", n_features, latent_dim)\n",
    "    vae_trainer.load_model(vae_output_path)\n",
    "    vae_preds = vae_trainer.predict(\n",
    "        X_test,\n",
    "        outlier_type=\"instance\",  # use 'feature' or 'instance' level\n",
    "        return_instance_score=True,  # Score used to determine outliers\n",
    "    )\n",
    "\n",
    "    # evaluate on the test data\n",
    "    f1_score_value = vae_trainer.evaluate(\n",
    "        y_test, vae_preds[\"data\"][\"is_outlier\"]\n",
    "    )\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"name\": \"f1_score\",  # The name of the metric. Visualized as the column name in the runs table.\n",
    "                \"numberValue\": f1_score_value,  # The value of the metric. Must be a numeric value.\n",
    "                \"format\": \"RAW\",  # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_metric(\"f1_score\", f1_score_value)\n",
    "        \n",
    "    return [json.dumps(metrics)]\n",
    "\n",
    "\n",
    "evaluate_op = func_to_container_op(\n",
    "    func=evaluate,\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"alibi-detect[tensorflow]==0.11.1\",\n",
    "        \"alibi==0.9.1\",\n",
    "        \"cloudpickle==2.1.0\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "        \"joblib==1.3.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"kfp==1.8.22\",\n",
    "        \"urllib3==1.26.15\",\n",
    "        \"requests-toolbelt==0.10.1\",  # To fix ImportError: cannot import name 'appengine' from 'urllib3.contrib'\n",
    "        \"mlflow==2.9.2\"\n",
    "    ],\n",
    "    modules_to_capture=[\n",
    "        \"intrusion_detection_model\",\n",
    "        \"constants\"\n",
    "    ],  # https://kubeflow-pipelines.readthedocs.io/en/1.8.13/source/kfp.components.html#kfp.components.func_to_container_op\n",
    "    use_code_pickling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d41eb7-5d26-49eb-b94b-55edaeadc6d1",
   "metadata": {},
   "source": [
    "## Define some pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e4ae99-a90a-4478-89ab-4c48c8d557ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"Intrusion detection training\", description=\"Intrusion detection.\")\n",
    "def intrusion_detection_pipeline():\n",
    "    prepare_data_task = prepare_data_op()\n",
    "    train_task = train_op(\n",
    "        run_id=prepare_data_task.outputs[\"Output\"],\n",
    "        x_train=prepare_data_task.outputs[\"X_train\"],\n",
    "        y_train=prepare_data_task.outputs[\"y_train\"],\n",
    "    )\n",
    "    print(train_task.outputs)\n",
    "    evaluate_task = evaluate_op(\n",
    "        run_id=train_task.outputs[\"run_id\"],\n",
    "        n_features=train_task.outputs[\"n_features\"],\n",
    "        mean=prepare_data_task.outputs[\"mean\"],\n",
    "        stdev=prepare_data_task.outputs[\"stdev\"],\n",
    "        vae_output=train_task.outputs[\"vae_output\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d5cc0-7725-48b5-8cae-9f592f2f363c",
   "metadata": {},
   "source": [
    "## Run the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643ed18b-903e-46ee-9349-7f17b3efc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token to authenticate to the `ml-pipeline` service\n",
    "session_cookie = get_session_cookie()\n",
    "\n",
    "# Initialize the client\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891bb68b-788a-4653-9b05-281db135393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vae_output': {{pipelineparam:op=train;name=vae_output}}, 'run_id': {{pipelineparam:op=train;name=run_id}}, 'n_features': {{pipelineparam:op=train;name=n_features}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.10.10.10:8080/pipeline/#/experiments/details/57b8126f-b0a7-4ca5-8431-624ff16c2cab\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.10.10.10:8080/pipeline/#/runs/details/4b61d015-20cb-44e9-ab21-aba189f4abc4\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=4b61d015-20cb-44e9-ab21-aba189f4abc4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(\n",
    "    intrusion_detection_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
